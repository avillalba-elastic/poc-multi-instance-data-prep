{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch inference with Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to provide an example on how to run batch inference with Bedrock. \n",
    "\n",
    "With batch inference, you can process a larger number of prompts in a more efficient way. Also, according to [pricing](https://aws.amazon.com/bedrock/pricing/), it is ~50% cheaper than doing inference with single requests.\n",
    "\n",
    "Note: Not all the models support batch inference. Check [here](https://docs.aws.amazon.com/bedrock/latest/userguide/batch-inference-supported.html) the ones that do.\n",
    "\n",
    "Note 2: SLA for Bedrock Batch Inference Jobs to start is ~24h. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create some sample data for demonstration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils.llm.input_builder import Message\n",
    "\n",
    "SYSTEM_PROMPT = \"Please respond only with emoji.\"\n",
    "messages = [\n",
    "    [Message(role=\"user\", content=\"Hello there! How are you doing today?\")],\n",
    "    [Message(role=\"user\", content=\"My cat is so cute!\")],\n",
    "    [Message(role=\"user\", content=\"I'm programming today\")],\n",
    "] * 100  # there is a requirement in Bedrock for having min 100 input samples\n",
    "\n",
    "llm_inputs = iter(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference batch job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way batch inference works with Bedrock is as follows:\n",
    "- You create (or already have) a dataset with your prompts in an S3 bucket.\n",
    "- You run the batch inference job. \n",
    "- The results are written in an S3 bucket. Then, if needed, you can download them.\n",
    "\n",
    "We have abstractions to run all these operations easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define our BedrockBatchInference instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from ml_utils.aws.bedrock.bedrock_batch_inference import BedrockBatchInference\n",
    "from ml_utils.llm.input_builder import ClaudeInputBuilder\n",
    "\n",
    "bedrock_batch_inference = BedrockBatchInference(\n",
    "    llm=ClaudeInputBuilder(),\n",
    "    out_path=Path(\"outputs\"),\n",
    "    in_uri=\"s3://ml-rd-bedrock-datasets/examples.jsonl\",  # the input data will be written to this file\n",
    "    out_uri=\"s3://ml-rd-bedrock-inference-outputs/\",  # the output data will be written to this file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's run the batch job with some inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-19 12:36:31.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mml_utils.aws.s3\u001b[0m:\u001b[36mupload_file_to_s3\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mexamples.jsonl already exists in s3://ml-rd-bedrock-datasets\u001b[0m\n",
      "\u001b[32m2025-02-19 12:36:32.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mml_utils.aws.s3\u001b[0m:\u001b[36mupload_file_to_s3\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mOverwritten /var/folders/qv/xp56l9w94j7bwwvff2yf03680000gn/T/tmpavklcszx to s3://ml-rd-bedrock-datasets/examples.jsonl\u001b[0m\n",
      "\u001b[32m2025-02-19 12:36:33.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mml_utils.aws.bedrock.bedrock_batch_inference\u001b[0m:\u001b[36m_run\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mSuccessfully launched job with ARN: arn:aws:bedrock:us-east-1:879381254630:model-invocation-job/wh68xt43r2ff\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "bedrock_batch_inference.run(\n",
    "    system=SYSTEM_PROMPT,\n",
    "    inputs=llm_inputs,\n",
    "    modelid=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    detached=True,  # not waiting for the job to finish\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can go to the Bedrock UI and check your job under Inference and Assessment > Batch inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have run the job in detach mode, the execution will not wait until it completes. However, if you set detached=True when running the job, the execution will automatically wait and download the results to the provided ouput path (in our case, outputs/). Of course, this step is not recommended if the dataset is huge, as downloading the results might take lots of time and storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More resources\n",
    "\n",
    "- [Example from AWS](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/introduction-to-bedrock/batch_api/batch-inference-transcript-summarization.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
